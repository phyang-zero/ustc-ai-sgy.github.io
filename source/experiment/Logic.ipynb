{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑回归实验\n",
    "\n",
    "## Tips\n",
    "- 实验前查看issue，同学们有问题可以在issue中提出，同时可以参考过往的问题完成此实验。\n",
    "- 善用搜索引擎、AIGC工具拓展知识面。\n",
    "## Issue\n",
    "[【腾讯文档】逻辑回归-issues](\n",
    "https://docs.qq.com/doc/DWmltZUxvRkNWY0FK)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 数据处理\n",
    "\n",
    "为了能够完成各种数据操作，我们需要某种方法来存储和操作数据。 通常，我们需要做两件重要的事：（1）获取数据；（2）将数据读入计算机后对其进行处理。 如果没有某种方法来存储数据，那么获取数据是没有意义的。\n",
    "\n",
    "### 预处理\n",
    "\n",
    "为了能用深度学习来解决现实世界的问题，我们经常从预处理原始数据开始， 而不是从那些准备好的张量格式数据开始。 在Python中常用的数据分析工具中，我们通常使用pandas软件包。 像庞大的Python生态系统中的许多其他扩展包一样，pandas可以与张量兼容。 这一节同学们讲使用pandas预处理原始数据，并将原始数据转换为张量格式的步骤。\n",
    "#### 数据集读取\n",
    "\n",
    "首先，需要尝试使用pandas中的`read_csv`函数读取train, val, test 这三个数据集（保留列名），同时在`utils.py`文件中，助教定义了一个名为`data_utils`的类，可以对读取的pd数据框进行简单的预处理。请尝试调用这个类的方法对于数据集进行进一步的处理。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 数据集读取\n",
    "# TODO\n",
    "train = \n",
    "val = \n",
    "test = \n",
    "\n",
    "# 数据集预处理\n",
    "# TODO\n",
    "train = \n",
    "val = \n",
    "test = \n",
    "\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 处理缺失值\n",
    "注意，上一个单元格输出的结果代表着每一列中缺失值的数量。为了处理缺失的数据，典型的方法包括插值法和删除法， 其中插值法用一个替代值弥补缺失值，而删除法则直接忽略缺失值。\n",
    "\n",
    "首先，我们对于需要分析数据的input 和 output 的label，注意到`isDefault`是output的列名，所以首先我们需要丢弃掉所有`isDefault`列为NaN的行。\n",
    "\n",
    "其次，对于其他input为空的行，我们需要使用同一列的均值来对 `NaN`进行填充。\n",
    "\n",
    "##### data.mean()\n",
    "用于计算数据框中每一列的均值。它可以帮助我们快速了解数据的中心趋势，特别是在处理缺失值时，我们可以用均值来填充NaN值。\n",
    "\n",
    "##### pd.fillna()\n",
    "是pandas中用于填充缺失值的方法。它允许我们用特定的值（如均值、中位数或其他值）来替换NaN。使用fillna()可以确保数据的完整性，避免在后续分析中因缺失值而导致的错误。\n",
    "\n",
    "##### pd.dropna()\n",
    "是pandas中用于删除缺失值的方法。它可以根据指定的条件（如某一列是否为NaN）来删除行或列。使用dropna()可以帮助我们清理数据集，确保分析时只使用完整的数据。\n",
    "\n",
    "##### 例如，以下代码展示了如何使用这些方法：\n",
    "```python\n",
    "1. 计算均值并填充缺失值：\n",
    "mean_value = data['column_name'].mean()\n",
    "data['column_name'].fillna(mean_value, inplace=True)\n",
    "\n",
    "2. 删除特定列中含有NaN的行：\n",
    "data.dropna(subset=['column_name'], inplace=True)\n",
    "```\n",
    "\n",
    "<!-- ##### 通过这些方法，我们可以有效地处理数据中的缺失值，提高数据分析的准确性。 -->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 过滤标签（`isDefault`列）为空的行\n",
    "\n",
    "def filter_NA(data):\n",
    "    \"\"\"\n",
    "    删除 `isDefault` 列为 NaN 的行\n",
    "    Args:\n",
    "        data (pd.DataFrame): 输入数据框\n",
    "    Returns:\n",
    "        pd.DataFrame: 过滤后的数据框\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    pass\n",
    "train, val, test = filter_NA(train), filter_NA(val), filter_NA(test)\n",
    "# 填充对于其他input为空的行\n",
    "def fill_mean(data):\n",
    "    \"\"\"\n",
    "    用每列均值填充 NaN\n",
    "    Args:\n",
    "        data (pd.DataFrame): 输入数据框\n",
    "    Returns:\n",
    "        pd.DataFrame: 填充后的数据框\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    pass\n",
    "train, val, test = fill_mean(train), fill_mean(val), fill_mean(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 转换为张量格式\n",
    "\n",
    "按照之前的划分方式，我们把数据划分成input和label，其中，label为isDefault 列，input为其他列，并且转换为张量(tensor)的形式。\n",
    "\n",
    "\n",
    "##### 1. Pandas数据选择和处理\n",
    "\n",
    "###### 1.1 数据选择\n",
    "在Pandas中，我们可以通过多种方式选择数据：\n",
    "\n",
    "```python\n",
    "# 假设我们有一个数据框df\n",
    "# 1. 选择单列\n",
    "column_data = df['column_name']  # 返回Series\n",
    "# 2. 选择多列\n",
    "columns_data = df[['column1', 'column2']]  # 返回DataFrame\n",
    "```\n",
    "\n",
    "###### 1.2 使用drop删除列\n",
    "`drop()`是一个非常有用的方法，用于删除指定的行或列：\n",
    "\n",
    "```python\n",
    "# 删除列\n",
    "df.drop('column_name', axis=1)  # axis=1表示删除列，axis=0表示删除行\n",
    "\n",
    "# 示例：删除'isDefault'列\n",
    "train_input = train.drop('isDefault', axis=1)\n",
    "```\n",
    "\n",
    "参数说明：\n",
    "- `axis=1`: 表示删除列\n",
    "- `axis=0`: 表示删除行\n",
    "- `inplace=True`: 如果设置为True，则直接在原数据框上修改\n",
    "\n",
    "###### 1.3 .values属性\n",
    "`.values`属性用于将Pandas的DataFrame或Series转换为NumPy数组：\n",
    "\n",
    "```python\n",
    "# DataFrame转换为NumPy数组\n",
    "numpy_array = df.values\n",
    "\n",
    "# 示例\n",
    "train_input_array = train_input.values  # 转换为numpy数组\n",
    "```\n",
    "\n",
    "##### 2. PyTorch张量转换\n",
    "\n",
    "###### 2.1 创建张量\n",
    "`torch.tensor()`用于创建PyTorch张量：\n",
    "\n",
    "```python\n",
    "# 基本语法\n",
    "tensor = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "# 示例：将numpy数组转换为PyTorch张量\n",
    "train_input = torch.tensor(train_input.values, dtype=torch.float32)\n",
    "```\n",
    "\n",
    "主要参数：\n",
    "- `data`: 输入数据（可以是列表、NumPy数组等）\n",
    "- `dtype`: 指定数据类型，常用的有：\n",
    "  - `torch.float32`: 32位浮点数\n",
    "  - `torch.int64`: 64位整数\n",
    "  - `torch.bool`: 布尔值\n",
    "\n",
    "##### 注意事项\n",
    "1. 在转换为张量时，要确保数据类型正确\n",
    "2. 对于分类问题的标签，通常使用`float32`或`long`类型\n",
    "3. 在处理大数据集时，注意内存使用情况\n",
    "4. 使用`drop()`时注意保存结果，因为默认不会修改原数据框\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 数据集划分\n",
    "\n",
    "train_input, train_label = \n",
    "val_input, val_label =\n",
    "test_input, test_label =\n",
    "\n",
    "\n",
    "# 使用MinMaxScaler对训练数据、验证数据和测试数据进行归一化处理\n",
    "# MinMaxScaler将特征缩放到指定的范围（默认是0到1），\n",
    "# 通过这种方式，可以确保每个特征在同一尺度上，从而提高模型的训练效果\n",
    "# 归一化处理有助于加快收敛速度，并减少不同特征之间的影响\n",
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "train_input = scaler.fit_transform(train_input.values)\n",
    "test_input = scaler.transform(test_input.values)\n",
    "val_input = scaler.transform(val_input.values)\n",
    "train_label = train_label.values\n",
    "test_label = test_label.values\n",
    "val_label = val_label.values\n",
    "\n",
    "# TODO 将数据转换为张量\n",
    "train_input = \n",
    "train_label = \n",
    "test_input = \n",
    "test_label = \n",
    "val_input = \n",
    "val_label = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建Pytorch数据集\n",
    "\n",
    "在pytorch中，提供了一种非常方便的数据读取机制，即`torch.utils.data.Dataset`类和`torch.utils.data.Dataloader`类，通过两种方式的组合，我们可以得到数据迭代器，在每次训练中，利用这个迭代器每次输出一组batch数据（即训练数据集中的一个子集）。\n",
    "\n",
    "#### torch.utils.data.Dataset\n",
    "torch.utils.data.Dataset代表着自定义数据集方法的类，用户可以继承这个类来自定义自己的数据集类，继承时，用户需要重载`__len__()`和 `__getitem__()`这两个方法\n",
    "\n",
    "- `__len__()` 返回数据集的大小。 我们构建的数据集也是一个对象，这个方法希望像list,string,tuple等方法一样，可以直接获得对象的大小\n",
    "- `__getitem__()`实现获得数据集中的某一个数据。 list,string,tuple可以通过一个索引来获得序列中的任意元素，通过实现``__getitem__()``，我们希望获得类似的功能。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, *tensors):\n",
    "        assert all(\n",
    "            tensors[0].size(0) == tensor.size(0) for tensor in tensors\n",
    "        ), \"Size mismatch between tensors\"\n",
    "        self.tensor = tensors\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Return a tuple of tensors at the given index\n",
    "        # Each tensor in self.tensor is sliced at the index position\n",
    "        # For example, if we have input tensor and label tensor\n",
    "        # This will return (input[index], label[index])\n",
    "        # TODO\n",
    "\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the length of the dataset\n",
    "        # TODO\n",
    "\n",
    "        pass\n",
    "\n",
    "train_dataset = MyDataset(train_input, train_label)\n",
    "val_dataset = MyDataset(val_input, val_label)\n",
    "test_dataset = MyDataset(test_input, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.utils.data.DataLoader\n",
    "作用：\n",
    "- `DataLoader`将`Dataset`对象或者自定义数据类封装成一个迭代器。\n",
    "- 迭代器可以迭代输出`Dataset`的内容\n",
    "- 可以实现多个进程、shuffle、不同采样策略，数据校对处理过程。\n",
    "\n",
    "`__init()__`中的几个重要的输入\n",
    "- `dataset`: pytorch已有的数据读取接口，或者自定义的数据接口的输入，该输出要么是torch.utils.data.Dataset类的对象，要么是自定义的类的对象\n",
    "- `batch_size`: 一个batch输出数量的多少。\n",
    "- `shuffle`: 是否随机打乱顺序，一般在训练数据中进行打乱。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 构建dataload，batch_size设置成64，只打乱train 的数据\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = \n",
    "val_loader = \n",
    "test_loader = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归\n",
    "\n",
    "回归（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。 在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。\n",
    "\n",
    "在机器学习领域中的大多数任务通常都与预测（prediction）有关。 当我们想预测一个数值时，就会涉及到回归问题。 常见的例子包括：预测价格（房屋、股票等）、预测住院时间（针对住院病人等）、 0-1变量预测（预测是否贷款）。 \n",
    "\n",
    "为了解释线性回归，我们举一个实际的例子：需要根据贷款申请人的数据信息预测其是否有违约的可能，以此判断是否通过此项贷款。为了开发一个能预测是否有违约的可能的模型，我们需要收集一个真实的数据集。这个数据集包括了贷款申请人的各种数据信息。在机器学习的术语中，该数据集称为训练数据集（training data set） 或训练集（training set）。 每行数据（比如一个用户相对应的数据）称为样本（sample）， 也可以称为数据点（data point）或数据样本（data instance）。 我们把试图预测的目标（比如是否违约）称为标签（label）或目标（target）。 预测所依据的自变量（面积和房龄）称为特征（feature）。\n",
    "\n",
    "通常，我们使用$n$来表示数据集中的样本数。 对索引为$i$的样本，其输出表示为$\\mathbf{x}^{(i)} = [x_1^{(i)},\\dots,x_n^{(i)}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线形模型\n",
    "线性假设是指目标可以表示为特征的加权和:\n",
    "$$ logit = \\mathbf{x}^{(i)} \\cdot \\mathbf{w} + b $$ \n",
    "其中$ \\mathbf{w} = [w_1^{(i)},\\dots,w_n^{(i)}] $ 称为权重，权重决定了每个特征对我们预测值的影响。 b称为偏置（bias）、偏移量（offset）或截距（intercept）。 偏置是指当所有特征都取值为0时，预测值应该为多少。给定一个数据集，我们的目标是寻找模型的权重$\\mathbf{w}$和偏置$b$， 使得根据模型做出的预测大体合理。\n",
    "\n",
    "### 逻辑函数(Logistic，也称为sigmoid,logit)\n",
    "可以发现，线性假设输出值无界，所以我们需要把线形输出值映射到输出概率$[0,1]$之间，参考[逻辑回归ppt](https://ustc-ai-sgy.github.io/slides/2.5%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%921.pdf)sigmoid函数部分\n",
    "\n",
    "### 损失函数\n",
    "\n",
    "在我们开始考虑如何用模型拟合（fit）数据之前，我们需要确定一个拟合程度的度量。 损失函数（loss function）能够量化目标的实际值与预测值之间的差距。 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。\n",
    "\n",
    "在这个实验部分，我们选择最大对数最大似然估计，对于所有权重（参数组合），我们选择一组参数，使得预测似然性最大。\n",
    "\n",
    "#### 极大似然估计\n",
    "在统计学中，极大似然估计（Maximum Likelihood Estimation）是用来估计模型参数的一种方法，就是利用已知样本的结果信息，反推出最有可能导致这样结果的模型参数值。简而言之，最大似然估计旨在找到能使已知数据最“自然”、最“合理”的模型参数。\n",
    "\n",
    "拓展阅读：[花书5.5](https://www.deeplearningbook.org/contents/ml.html)\n",
    "\n",
    "#### 对数最大似然估计\n",
    "\n",
    "由于本次实验相当于二分类问题，模型预测一个数据点为正样本的概率为:\n",
    "$$\\hat{y}_i = \\hat{P}_{positive}(i) = \\frac{1}{1+e^{ -logit}} $$\n",
    "预测其为负样本的概率为:\n",
    "$$\\hat{P}_{negetive}(i) = \\frac{e^{-logit}}{1+e^{ -logit}} $$\n",
    "\n",
    "结合最大似然估计，我们希望样本的预测概率尽可能接近真实标签。对于二分类问题，交叉熵损失函数（Cross-Entropy Loss）通常用于衡量预测概率与真实标签之间的差异。交叉熵损失函数定义如下：\n",
    "\n",
    "$$L(y, \\hat{y}) = -\\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)]$$\n",
    "\n",
    "其中，$y_i$ 是第 $i$ 个样本的真实标签，$\\hat{y}_i$ 是第 $i$ 个样本预测为正样本的概率，$N$ 是样本总数。\n",
    "\n",
    "\n",
    "#### torch.nn.BCELoss\n",
    "\n",
    "torch.nn.BCELoss 是PyTorch中用于计算二分类交叉熵损失的类。\n",
    "它的数学公式为:\n",
    "$$BCE(x, y) = -[y * log(x) + (1 - y) * log(1 - x)]$$\n",
    "\n",
    "其中:\n",
    "- x: 模型预测的概率值,范围在[0,1]之间\n",
    "- y: 真实标签,值为0或1\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, norm='l2'):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        assert norm in ['l1', 'l2'] \n",
    "        self.w = nn.Parameter(torch.randn(input_dim, 1))\n",
    "        self.b = nn.Parameter(torch.randn(1))\n",
    "        self.norm=norm\n",
    "\n",
    "\n",
    "    def l1_norm(self):\n",
    "        # return l1 norm as a loss\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def l2_norm(self):\n",
    "        # return l2 norm as a loss\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def normnize(self):\n",
    "        # return l1 or l2 norm as a loss\n",
    "        if self.norm == 'l1':\n",
    "            return self.l1_norm()\n",
    "        else:\n",
    "            return self.l2_norm()\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        # return sigmoid of x\n",
    "        # TODO\n",
    "        pass    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # return logits and call sigmoid function.\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "def criterion(y_pred, y_true):\n",
    "    # 计算二元交叉熵损失\n",
    "    # y_pred: 模型预测的概率值,范围在[0,1]之间\n",
    "    # y_true: 真实标签,值为0或1\n",
    "    # 返回: 损失值\n",
    "    # compute loss \n",
    "    loss_fn = nn.BCELoss()\n",
    "    \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "input_dim = train_input.size(1)\n",
    "model = LogisticRegression(input_dim, norm='l1')\n",
    "lr = 0.005\n",
    "num_epochs = 100\n",
    "norm_weight = 0.005\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    \n",
    "    val_total = 0\n",
    "    val_correct = 0\n",
    "    for val_input, val_label in val_loader:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(val_input)\n",
    "            predicted = (val_pred >= 0.5).float()\n",
    "            val_total += val_label.size(0)\n",
    "            val_correct += (predicted.view(-1) == val_label).sum().item()\n",
    "    val_accuracy = val_correct / val_total\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for train_input, train_label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(train_input)\n",
    "        loss = criterion(pred, train_label)\n",
    "        norm_loss = model.normnize()\n",
    "        loss = loss + norm_loss*norm_weight\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_avg = train_loss / len(train_loader)\n",
    "    print(f\"Epoch: {epoch}, Train Loss: {train_avg}\")\n",
    "\n",
    "    print(f\"Epoch: {epoch}, Val Accuracy: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_total = 0\n",
    "test_correct = 0\n",
    "for test_input, test_label in test_loader:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_pred = model(test_input)\n",
    "        predicted = (test_pred >= 0.5).float()\n",
    "        test_total += test_label.size(0)\n",
    "        test_correct += (predicted.view(-1) == test_label).sum().item()\n",
    "test_accuracy = test_correct / test_total\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logitic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
